{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd7dc10-2ee1-4493-bff0-4d6e529c48e5",
   "metadata": {},
   "source": [
    "# A Perceptron Model with Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8516097-1e90-4cb5-adff-285d0d2a316d",
   "metadata": {},
   "source": [
    "In this exercise, you are going to modify the `Perceptron` class such that it creates the predictions for multiple input examples at once. \n",
    "\n",
    "Suppose you have the following perceptron model, which is a simplified version of the `Perceptron` model we have seen in Unit 2.6. (It is identical, except that the `update` method was removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d7e8f9-191b-4d24-80a9-e5b8b278aee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.weights = torch.tensor([2.86, 1.98])\n",
    "        self.bias = torch.tensor(-3.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_sum_z = torch.dot(x, self.weights) + self.bias\n",
    "\n",
    "        if weighted_sum_z > 0.0:\n",
    "            prediction = torch.tensor(1.0)\n",
    "        else:\n",
    "            prediction = torch.tensor(0.0)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be62a6-5651-4777-b2d1-3eeaa9d06b4f",
   "metadata": {},
   "source": [
    "The perceptron above has hard-coded weights -- suppose these were the result from training it on a training dataset.\n",
    "\n",
    "Now, suppose we want to use it for making predictions on new data, `X_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1a297f4-74a4-4207-a845-cc8037a9b075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_data = torch.tensor([\n",
    "    [-1.0, -2.0],\n",
    "    [-3.0, 4.5],\n",
    "    [5.0, 6.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f9d6e-d7d7-466b-8557-7ff728a99a2b",
   "metadata": {},
   "source": [
    "Using the concepts from Unit 2.6, we can use a for-loop to obtain the prediction for these 3 data examples as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d92b815-1c6f-4cbb-a60e-ab869b2612a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "ppn = Perceptron()\n",
    "\n",
    "for x in X_data:\n",
    "    print(ppn.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db12582-b305-4f25-9ac3-ca42e374516d",
   "metadata": {},
   "source": [
    "We learned that using for-loops can be very inefficient. So, we want to avoid this for loop, which we can achieve by modifying the `Perceptron` class above. \n",
    "\n",
    "<font color='red'>For this, you will need to modify the `forward` \n",
    "    method in 2 ways:</font>\n",
    "\n",
    "<font color='red'>1. Use `.matmul` instead of `.dot`</font>\n",
    "\n",
    "<font color='red'>2. Use `torch.where` (see Unit 2, Exercise 1)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdb04e07-8772-410b-ba42-c390bbcd091b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.weights = torch.tensor([2.86, 1.98])\n",
    "        self.bias = torch.tensor(-3.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_sum_z = np.matmul(x,self.weights) + self.bias\n",
    "        prediction = torch.where(weighted_sum_z > 0.,1.,0.)\n",
    "        return prediction\n",
    "    \n",
    "    def update(self, x, true_y):\n",
    "        prediction = self.forward(x)\n",
    "        error = true_y - prediction\n",
    "\n",
    "        # update\n",
    "        self.bias += error\n",
    "        self.weights += error * x\n",
    "\n",
    "        return error \n",
    "            \n",
    "        ### YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae5d16-b7d4-49cc-9c03-9b4074a75af3",
   "metadata": {},
   "source": [
    "After modifying the forward method above, you should get the same results as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c479cca-12a9-4655-9c30-cbbe9af2f9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppn = Perceptron()\n",
    "ppn.forward(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf9642-0ca1-42d0-a2b7-b48bfebc3c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
